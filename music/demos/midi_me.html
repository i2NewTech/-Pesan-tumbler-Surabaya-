<!-- Copyright 2019 Google Inc. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================-->
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1, user-scalable=yes">
  <link rel="stylesheet" href="./style.css">
  <title>MidiME</title>
  <style>
    input[type="file"] {
      width: 0;
      height: 0;
      opacity: 0;
      cursor: pointer;
      display: none;
    }
  </style>
</head>
<body>
  <h1>MidiME</h1>
  <p><code>MidiME</code> is a variational autoencoder that lets you personalize
    your own <code>MusicVAE</code> model with just a little data, so that samples
    from <code>MidiME</code> sound closer to your provided data. Normally training
    a <code>MusicVAE</code> model with new data would take days on a big GPU and
    millions of data points, but with <code>MidiME</code> you can do it
    with a single MIDI file, and directly in the browser. </p>
  <h2>Input data</h2>
  <p><code>MidiME</code> works in parallel with a <code>MusicVAE</code> model,
  so the kind of results you get depend on which <code>MusicVAE</code> checkpoint
  you use. Here, we give examples of both training on melodies (the "monophonic"
  section), and multi-instrument trios (the "polyphonic" section).</p>
  <h2>1. Monophonic models</h2>
  <section>
    <p>This is what you want the samples to sound like. For this example we
      are using the <code>mel_2bar_small</code> checkpoint, which is
      monophonic -- if you use a polyphonic MIDI file, you're not
      guaranteed to reconstruct the "main" melody, just a single instrument.</p>
      <p>Try training on a single full song to get outputs that sound like
        variations on it, or train on multiple songs to get samples that
        combine various characteristics of them.</p>
    <label class="button" id="mel_fileBtn" disabled>
      Load midi file(s)
      <input type="file" id="mel_fileInput" multiple>
    </label>
    <p><code id="mel_input"></code>
  </section>

  <h2>Training</h2>
  <p>The MidiMe model works by training a variational autoencoder in the
    browser. The more steps you train for, the better the input reconstruction will be.
    <button id="mel_train" disabled>start training</button>
  </p>
  <section>
    <svg id="mel_graph"></svg>
  </section>

  <h2>Input reconstruction</h2>
  <section>
    <p><b>Before training:</b> <code id="mel_pre-training"></code></p>
    <p><b>After training:</b> <code id="mel_post-training"></code></p>
    <p><b>It took:</b> <code id="mel_training-time"></code></p>
  </section>

  <h2>Random samples</h2>
  <p>You can now sample from MidiMe. If you trained for long enough, then
    these samples will be very si